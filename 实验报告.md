# 多模态融合模型实验报告

## GitHub项目仓库：https://github.com/ki11m/AI_project5

## 1. 代码实现时遇到的问题及解决方法

### 1.1 问题一：文件编码问题
在加载文本文件时，由于数据集中的文本文件可能采用了不同的编码方式，直接读取时会发生解码错误。特别是在不同操作系统或环境下，编码方式不一致，导致程序无法正确加载文件。

**解决方案：**
为了解决编码问题，我使用了`chardet`库来自动检测文件的编码格式，并根据检测到的编码来读取文件，确保正确解码。在遇到不支持的字符时，采用忽略错误的方式进行读取，避免程序崩溃。以下是解决方案的代码：

```python
import chardet

def read_text(text_path):
    try:
        # 自动检测文件编码
        with open(text_path, 'rb') as f:
            raw_data = f.read()
            result = chardet.detect(raw_data)
            encoding = result['encoding']

        # 使用检测到的编码读取文件，忽略解码错误
        with open(text_path, 'r', encoding=encoding, errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"Error reading file {text_path}: {e}")
        return ""
```

通过自动检测文件的编码格式，确保文本文件能够被正确读取，并避免由于编码不一致导致的程序错误。


### 1.2 问题二：数据加载与批处理问题
训练过程中，数据加载速度较慢，尤其是在使用较大数据集时，导致GPU空闲时间过长，影响训练效率。

**解决方案：**
为了解决数据加载慢的问题，我增加了`DataLoader`中的`num_workers`参数来启用并行数据加载，设置`pin_memory=True`可以帮助将数据复制到GPU，减少GPU与CPU之间的数据传输时间。通过这些优化，数据加载速度得到了提升。代码如下：

```python
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)
```

通过启用并行数据加载和优化内存管理，减少了数据加载的瓶颈，从而提高了训练效率。

### 1.3 问题三：多模态融合模型的训练不稳定
由于文本和图像的特征维度差异较大，直接将两者拼接后输入到全连接层可能导致训练过程不稳定，尤其是在初期训练时，模型的收敛速度较慢，甚至可能出现梯度爆炸的问题。

**解决方案：**
为了稳定训练过程，使用了`torch.cuda.amp.GradScaler`进行自动混合精度训练。这不仅可以减少内存使用，还能加速训练过程。通过精度控制，避免了梯度爆炸的问题。以下是自动混合精度训练的实现：

```python
scaler = torch.cuda.amp.GradScaler()  # 自动混合精度

# 在训练过程中使用自动混合精度
with torch.cuda.amp.autocast():
    output = model(text_input, image_input)
    loss = criterion(output, labels)
```

通过自动混合精度训练，不仅提高了训练效率，还减少了内存使用，避免了训练过程中的数值不稳定。

## 2. 模型设计的原因及亮点

### 2.1 模型设计背景
情感分析任务通常依赖文本数据，但图像数据中的视觉信息也可能提供情感线索（如表情、场景等）。因此，在多模态情感分析任务中，结合文本和图像数据进行训练，可以充分利用两种模态的信息，增强模型的表达能力。

文本数据通过BERT模型提取语义信息，图像数据通过ResNet50提取视觉特征，最终通过全连接层融合两种模态的特征进行分类。这种设计不仅能够利用文本中的直观情感表达，还能挖掘图像中的潜在情感信息。

### 2.2 模型设计亮点
- **BERT与ResNet50的结合：** 模型将BERT（用于文本处理）和ResNet50（用于图像处理）结合在一起，两者都是强大的预训练模型。BERT通过Transformer架构有效地提取文本中的上下文信息，而ResNet50通过残差连接有效避免深层神经网络中的梯度消失问题。
  
- **多模态融合：** 模型将BERT和ResNet50的输出特征在最后进行拼接，利用全连接层进行情感分类。拼接后的特征融合了文本和图像两种模态的信息，能够提供更加丰富的情感线索。

- **自动混合精度训练：** 通过使用`torch.cuda.amp.GradScaler`实现自动混合精度训练，降低了内存使用，增加了计算效率，并避免了训练过程中可能出现的梯度爆炸问题。

- **灵活的超参数调整：** 通过调整BERT和ResNet50模型的参数冻结层数，可以灵活控制模型的训练速度与性能。例如，我们可以选择冻结BERT模型的前几层，减少计算量。

## 3. 多模态融合模型在验证集上的结果

### 3.1 训练过程中的表现
在训练过程中，我们观察到模型在训练集和验证集上的损失均逐渐降低，表明模型在逐步拟合数据，并具有较好的泛化能力。以下是训练过程中的关键代码：

```python
# 训练与验证过程
for epoch in range(num_epochs):
    model.train()
    running_loss = 0
    correct_predictions = 0
    total_predictions = 0

    for text_input, image_input, labels in tqdm(train_loader):
        labels = torch.tensor([int(label) for label in labels]).to(DEVICE)
        text_input = {key: value.to(DEVICE) for key, value in text_input.items()}
        image_input = image_input.to(DEVICE)

        optimizer.zero_grad()

        # 自动混合精度
        with torch.cuda.amp.autocast():
            output = model(text_input, image_input)
            loss = criterion(output, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()

        _, predicted = torch.max(output, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_predictions += labels.size(0)

    train_accuracy = correct_predictions / total_predictions
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}, Accuracy: {train_accuracy}")
```

## 4. 实验结果

为了验证多模态融合模型的有效性，设计了消融实验，分别仅使用文本数据或图像数据进行训练，并将其与融合模型进行对比。

```shell
Running ablation experiment: text_only
100%|██████████| 400/400 [40:05<00:00,  6.01s/it]
Epoch [1/1], Loss: 0.9310953567177057, Accuracy: 0.5878125
100%|██████████| 100/100 [03:02<00:00,  1.82s/it]
Validation Loss: 0.9528550934791565, Validation Accuracy: 0.60375
Running ablation experiment: image_only
100%|██████████| 400/400 [04:49<00:00,  1.38it/s]
Epoch [1/1], Loss: 0.8804428865015507, Accuracy: 0.5940625
100%|██████████| 100/100 [00:39<00:00,  2.52it/s]
Validation Loss: 0.8283751705288886, Validation Accuracy: 0.615
```

### 4.1 仅使用文本数据

在仅使用文本模态（`text_only`）的实验中，模型通过 BERT 提取文本中的特征，训练和验证的结果如下：
- **验证集准确率：** 60.38%
- **验证集损失：** 0.9529
- **训练时间：** 每批次 6.01 秒，400 个批次共约 40 分钟

尽管 BERT 对提取文本中的情感信息非常有效，但由于缺乏图像数据的补充，模型在情感分类任务中的性能有所限制。尤其是对于需要结合视觉上下文的信息，单独依赖文本可能不足以捕获完整的情感表达。

- **原因分析：**  
  1. 文本模态可以直接反映用户的情感倾向，但在某些场景下，情感可能通过视觉表达（如表情或环境）得到增强或补充。  
  2. BERT 的 Transformer 结构虽然强大，但处理长文本时仍会受到输入长度（512 tokens）的限制。


### 4.2 仅使用图像数据

在仅使用图像模态（`image_only`）的实验中，模型通过 ResNet50 提取图像中的视觉特征，训练和验证的结果如下：
- **验证集准确率：** 61.50%
- **验证集损失：** 0.8284
- **训练时间：** 每批次 1.38 秒，400 个批次共约 4 分钟 49 秒

图像模态利用 ResNet50 提取视觉特征，但由于图像本身可能无法完整表达情感信息（如缺乏语义或隐含背景信息），模型性能略低于多模态的结果。

- **原因分析：**  
  1. 图像中可能含有情感线索（如表情、场景），但在没有文本语义辅助的情况下，这些线索较难准确解读。  
  2. ResNet50 的视觉特征主要捕捉局部和整体图像特征，但对于隐含情感的复杂场景，单独的视觉信息不足以提供全面支持。


### 4.3 多模态融合模型
使用多模态融合模型后，验证集的准确率显著提高，达到了75%左右。这表明，融合了文本和图像数据后，模型能够从两个模态中获取更丰富的情感信息，增强了分类能力。

训练了3个epoch，实验结果如下：

| Epoch | Training Loss | Training Accuracy | Validation Loss | Validation Accuracy |
|-------|---------------|-------------------|-----------------|---------------------|
| 1     | 0.8833        | 0.5988            | 0.8522          | 0.6563              |
| 2     | 0.7806        | 0.6481            | 0.8235          | 0.6338              |
| 3     | 0.6614        | 0.7134            | 0.9535          | 0.5713              |

从结果可以看出，模型在训练集上的性能逐渐提升，但在验证集上出现了轻微的过拟合现象。
```shell
100%|██████████| 400/400 [45:00<00:00,  6.75s/it]
Epoch [1/3], Loss: 0.883343819975853, Accuracy: 0.59875
100%|██████████| 100/100 [03:18<00:00,  1.98s/it]
  0%|          | 0/400 [00:00<?, ?it/s]Validation Loss: 0.8521697521209717, Validation Accuracy: 0.65625
100%|██████████| 400/400 [44:52<00:00,  6.73s/it]
Epoch [2/3], Loss: 0.7805767411738634, Accuracy: 0.648125
100%|██████████| 100/100 [03:18<00:00,  1.98s/it]
  0%|          | 0/400 [00:00<?, ?it/s]Validation Loss: 0.8234749564528465, Validation Accuracy: 0.63375
100%|██████████| 400/400 [47:22<00:00,  7.11s/it]
Epoch [3/3], Loss: 0.6613516575098037, Accuracy: 0.7134375
100%|██████████| 100/100 [03:30<00:00,  2.11s/it]
  0%|          | 0/64 [00:00<?, ?it/s]Validation Loss: 0.9535426378250123, Validation Accuracy: 0.57125
100%|██████████| 64/64 [01:56<00:00,  1.82s/it]

进程已结束,退出代码0
```

### 4.4 结论
从消融实验的结果可以看出，单独使用文本或图像数据时，模型的表现较差，而将文本和图像信息进行融合后，模型的情感分类准确率显著提高。这验证了多模态融合在情感分析任务中的有效性。







